\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{stackrel}
\renewcommand{\theequation}{\arabic{equation}}
\newcounter{neq}
\providecommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\QED}{\hfill \textit{\textbf{Q.E.D.}}}
\author{Agustin Curto, agucurto95@gmail.com}
\title{Resumen de teoremas para el final \\ de Matemática Discreta  II}
\date{2016}

\begin{document}
\maketitle
\tableofcontents


\chapter{Parte A}

	\section{La complejidad de EDMONS-KARP}
		\textbf{\underline{Teorema:}} La complejidad de $\langle E-K \rangle$ con $n = \abs{V}$ y $m = \abs{E}$ es $\mathcal{O}(nm^{2})$.

		\textbf{\underline{Prueba:}} Sean: $f_{0}, f_{1}, f_{2}, \; \dotsc$ \; la sucesión de flujos creados por $\langle E-K \rangle$. Es decir, el paso \textit{k} crea $f_{k}$.
			\vspace{5mm}
			\par Para cada \textit{k} definimos funciones:
			\begin{itemize}
				\item $d_{k}(x) =$ \textquotedblleft distancia\textquotedblright \; entre \textit{s} y x en el paso \textit{k}, en caso de existir, si no $\infty$.
				\item $b_{k}(x) =$ \textquotedblleft distancia\textquotedblright \; entre x y \textit{t} en el paso \textit{k}, en caso de existir, si no $\infty$.
			\end{itemize}

			\textquotedblleft Distancia\textquotedblright: longitud del menor camino aumentante entre dos vértices.

			\vspace{5mm}
			\par Observaciones:
				\begin{enumerate}
					\item
						\begin{itemize}
							\item $d_{k}(s) = 0$
							\item $b_{k}(t) = 0$
						\end{itemize}
					\item Sabemos que las distancias de $\langle E-K \rangle$ no disminuyen en pasos sucesivos, como esto será útil para esta demostración llamaremos $\circledast$ a la demostración de:
						\[ d_{k}(x) \leq d_{k+1}(x) \]
						\[ b_{k}(x) \leq b_{k+1}(x) \]
				\end{enumerate}

			\par Llamemos \textit{\underline{crítico}} a un lado disponible en el paso \textit{k} pero no disponible en el paso \textit{k + 1}. Es decir, si \textit{xy} es un lado $\Rightarrow$ \textit{xy} se satura ó \textit{yx} se vacía en el paso \textit{k}.
			\par Supongamos que al construir $f_{k}$ el lado \textit{xy} se vuelve crítico, el camino: \textit{s} $\dotsc$ x, y $\dotsc$ \textit{t} se usa para construir $f_{k}$.
			\begin{eqnarray}
				d_{k}(\textit{t}) &=& d_{k}(x) + b_{k}(x) \\
				\nonumber &=& d_{k}(x) + b_{k}(y) + 1
			\end{eqnarray}

			\par Para que \textit{xy} pueda ser \textit{crítico} nuevamente debe ser usado en la otra dirección (\textit{i.e yx}). Sea \textit{j} el paso posterior a \textit{k} en el cual se usa el lado en la otra dirección, el camino \textit{s} $\dotsb$ y, x $\dotsb$ \textit{t} se usa para construir $f_{j}$.
			\begin{eqnarray}
				d_{j}(t) &=& d_{j}(x)+ b_{j}(x) \\
				\nonumber &=& d_{j}(y)+1+b_{j}(x)
			\end{eqnarray}

			\par Entonces:
			\begin{equation*}
				\textup{De (1) y (2)} \Rightarrow
	  		\left \lbrace
	  		\begin{array}{l}
					d_{k}(y) = d_{k}(x) + 1 \; \; \star \\
	    		d_{j}(x) = d_{j}(y) + 1 \; \; \dag
	  		\end{array}
	  		\right.
			\end{equation*}

			\par Luego:
			\begin{eqnarray}
				\nonumber d_{j}(t) &=& d_{j}(x) + b_{j}(x) \\
				\nonumber &=& d_{j}(y) + 1 + b_{j}(x) \qquad\qquad\qquad\text{Por } \dag \\
				\nonumber & \geq & d_{k}(y) + 1 + b_{k}(x) \qquad\qquad\qquad\text{Por} \circledast \\
				\nonumber &=& d_{k}(x) + 1 + 1 + b_{k}(x) \;\qquad\qquad\text{Por} \star \\
				\nonumber &=& d_{k}(\textit{t}) + 2 \\
				\nonumber \Rightarrow d_{j}(\textit{t}) & \geq & d_{k}(\textit{t}) + 2
			\end{eqnarray}

			\vspace{5mm}
			\par Por lo tanto cuando un lado se vuelve crítico recien puede volver a saturarse cuando la distancia de \textit{s} a \textit{t} haya aumentado en por lo menos 2. Puede existir $\mathcal{O}(n/t)$ tales aumentos, es decir:
				\[ \# \textup{ Veces que un lado puede volverse crítico } = \mathcal{O}(n). \]

			\begin{eqnarray}
				\nonumber \; \therefore Complejidad(\langle E-K\rangle) &=& (\# pasos) * Compl(1 \; \textit{paso}) \\
				\nonumber &=& (\# \textup{veces que un lado se vuelve crítico}) * (\# lados) * Compl(BFS) \\
				\nonumber  &=& \mathcal{O}(n) * \mathcal{O}(m)* \mathcal{O}(m) \\
				\nonumber &=& \mathcal{O}(nm^{2})
			\end{eqnarray}


	\section{Las distancias de Edmonds-Karp no disminuyen en pasos sucesivos}
		\textbf{\underline{Teorema:}} Sean: $f_{0}, f_{1}, f_{2}, \; \dotsc$ \; la sucesión de flujos creados por $\langle E-K\rangle$. Es decir, el paso \textit{k} crea $f_{k}$.

			\vspace{5mm}
			\par Para cada \textit{k} definimos funciones:

			\begin{itemize}
				\item $d_{k}(x) =$ \textquotedblleft distancia\textquotedblright \; entre \textit{s} y x en el paso \textit{k} en caso de existir, si no $\infty$.
				\item $b_{k}(x) =$ \textquotedblleft distancia\textquotedblright \; entre x y \textit{t} en el paso \textit{k} en caso de existir, si no $\infty$.
			\end{itemize}

			\textquotedblleft Distancia\textquotedblright: longitud del menor camino aumentante entre dos vértices.

			\vspace{5mm}
			\par Queremos probar que:
			\begin{enumerate}
				\item $d_{k}(x) \leq d_{k + 1}(x)$
				\item $b_{k}(x) \leq b_{k + 1}(x)$
			\end{enumerate}

		\textbf{\underline{Prueba:}} Lo probaremos por inducción y solo para $d_{k}$ ya que para $b_{k}$ la prueba es análoga.
			\[ \textup{HI: H(i) } = \lbrace\forall_{z}: d_{k + 1}(z) \leq \textit{i}, \textup{ vale } d_{k}(z) \leq d_{k + 1}(z)\rbrace \]

			\begin{enumerate}
				\item \underline{Caso Base:} \begin{tabular}{|c|} \hline i = 0 \\\hline \end{tabular} \qquad $H(0) = \lbrace\forall_{z}: d_{k + 1}(z) \leq 0, \textup{ vale } d_{k}(z) \leq d_{k + 1}(z)  \rbrace$
					\par Pero $d_{k + 1}(z) \leq 0 \Rightarrow z = \textit{s}$, entonces:
					\begin{eqnarray}
						\nonumber d_{k}(z) &=& d_{k}(s) \\
						\nonumber &=& 0 \\
						\nonumber &\leq & d_{k + 1}(s) \\
						\nonumber &\leq & d_{k + 1}(z) \\
						\nonumber \therefore d_{k}(z) & \leq & d_{k + 1}(z)
					\end{eqnarray}
				\item \underline{Caso Inductivo:} Supongamos ahora que vale H(\textit{i}), veamos que vale H(\textit{i + 1}).
					\par Sea \textit{z} con $d_{k + 1}(z) \leq \textit{i+1}$, si $d_{k + 1}(z) \leq \textit{i}$ vale H(\textit{i}) para \textit{z}.
					\par \begin{center} $\therefore \; d_{k}(z) \leq d_{k + 1}(z) $ \end{center}
					\par Supongamos que \begin{tabular}{|c|} \hline $d_{k + 1}(z) = i + 1$ \\\hline \end{tabular}
					\par Entonces existe un camino aumentante, relativo a $f_{k}$, de la forma: $s = z_{0}, \; z_{1}, \; \dotsc \; z_{i}, \; z_{i + 1} = z$.
					\par Sea \begin{tabular}{|c|} \hline $x = z_{i}$ \\ \hline \end{tabular}

					\begin{itemize}
						\item \underline{Caso 1:} Existe algun camino aumentante, relativo a $f_{k - 1}$ de la forma $s, \; \dotsc \; x, \; z$.
							\begin{center}
								$\therefore \begin{tabular}{|c|} \hline $d_{k}(z) \leq d_{k}(x) + 1$ \\ \hline \end{tabular}$
							\end{center}

							\vspace{2mm}
							\par Pues al haber un camino $\underbrace{s, \; \dotsc \; x,}_{d_{k}(x)} \; z$, llamemosle A, de longitud $d_{k}(x) + 1$ entre \textit{s} y \textit{z}, sabemos que el minimo de todos los caminos de \textit{s} a \textit{z} seran $\leq$ A.
						\item \underline{Caso 2:} No existe un camino aumentante, relativo a $f_{k - 1}$, pero si existe un camino aumentante relativo a $f_{k}$. Por lo tanto el lado \textit{xz} no esta \textquotedblleft disponible\textquotedblright \; en el paso \textit{k}, ya que \textit{xz} está saturado, o bien \textit{zx} está vacío relativo a $f_{k - 1}$. Para construir $f_{k}$ usamos un camino de la forma $s, \; \dotsc \; z, \; x$. Es decir:
							\begin{enumerate}[1)]
								\item $f_{k - 1}(\overrightarrow{xz}) = C(\overrightarrow{xz})$ pero $f_{k}(\overrightarrow{xz}) < C(\overrightarrow{xz}), \; f_{k}$ devuelve flujo por $\overrightarrow{xz}$ ó
								\item $f_{k - 1}(\overrightarrow{zx}) = 0$ pero $f_{k}(\overrightarrow{zx}) > 0, \; f_{k}$ manda flujo por $\overrightarrow{zx}$.
							\end{enumerate}

							\par Como $\langle E-K \rangle$ funciona con BFS, ese camino usado para construir $f_{k}$ debe ser de longitud mínima. Es decir:
							\begin{eqnarray}
								\nonumber d_{k}(x) &=& d_{k}(z) + 1 \\
								\nonumber d_{k}(z) &=& d_{k}(x) - 1 \\
								\nonumber &\leq & d_{k}(x) + 1
							\end{eqnarray}
					\end{itemize}

				\underline{Conclusión:} En cualquiera de los dos casos tenemos:
				\begin{center}
					\begin{tabular}{|c|} \hline $d_{k}(z) \leq d_{k}(x) + 1$ \\\hline \end{tabular}
				\end{center}

				\par Ahora bien:
				\begin{eqnarray}
					\nonumber d_{k + 1}(x) &=& d_{k + 1}(z_{i}) \\
					\nonumber &=& i \\
					\nonumber &\Rightarrow & \textup{H(\textit{i}) vale para x.} \\
					\nonumber \therefore \; d_{k}(z) & \leq & d_{k + 1}(x)
				\end{eqnarray}

				\par Por lo tanto:
				\begin{eqnarray}
					\nonumber d_{k}(z) &\leq & d_{k}(x) + 1 \\
					\nonumber &\leq & d_{k + 1}(x) + 1 \\
					\nonumber &=& i + 1 \\
					\nonumber &=& d_{k + 1}(z) \\
					\nonumber & \Rightarrow & \textup{H(i + 1) vale.}
				\end{eqnarray}
			\end{enumerate}


	\section{La complejidad de DINIC}
		\textbf{\underline{Teorema:}} La complejidad del algoritmo de Dinic es $\mathcal{O}(n^{2}m)$.

		\textbf{\underline{Prueba:}} Como Dinic es un algoritmo que trabaja con networks auxiliares y vimos que la distancia entre \textit{s} y \textit{t} en networks auxiliares consecutivos aumenta y puede ir a lo sumo entre 1 y $n - 1$ entonces hay a lo sumo $\mathcal{O}(n)$ networks auxiliares.
			\[ \textup{Complejidad(Dinic) } = \mathcal{O}(n) * \textup{ Compl(Hallar un flujo bloqueante en un NA con Dinic)} \]
			\par Para probar que la complejidad de Dinic es $\mathcal{O}(n^{2}m)$ debemos probar que complejidad del paso bloqueante es $\mathcal{O}(nm)$.
			\par Sea:
			\begin{itemize}
				\item A $=$ Avanzar()
				\item R $=$ Retroceder()
				\item I $=$ IncrementarFlujo() + Inicialización
			\end{itemize}

			\par Una corrida de Dinic luce como:
			\begin{center}
				AA$\; \dotsc \;$AIAAARA$\; \dotsc \;$AIAARAAARR$\; \dotsc \;$IA$\; \dotsc \;$
			\end{center}

			\par Dividamos la corrida en subpalabras del tipo:
			\begin{center}
				\begin{itemize}
					\item[$*$] $\underbrace{AA \; \dotsc \; A}_{Todas \; A's}I$
					\item[$*$] $\underbrace{AA \; \dotsc \; A}_{Todas \; A's}R$
				\end{itemize}
			\end{center}

			\par \textbf{Nota:} el número de A's puede ser 0.
			\vspace{5mm}
			\par Debemos determinar:
			\begin{enumerate}
				\item Cual es la complejidad de cada subpalabra.
				\item Cuantas palabras hay de cada tipo.
			\end{enumerate}

			\textbf{Complejidad de cada subpalabra}
				\par Recordemos que:
				\begin{equation*}
					\textup{A:}
		  		\left[
		  		\begin{array}{l}
		    		P[i + 1] = \textup{algún elemento de } \Gamma^{+}(P[i]) \\
		     		i = i + 1
		  		\end{array}
		  		\right.
				\end{equation*}

				\[ \Rightarrow \textup{ A es } \mathcal{O}(1) \]

				\begin{equation*}
					\textup{R:}
		  		\left[
		  		\begin{array}{l}
		    		\textup{Borrar } P[i - 1]P[i] \textup{ del NA} \\
		     		i = i - 1
		  		\end{array}
		  		\right.
				\end{equation*}

				\[ \Rightarrow \textup{ R es } \mathcal{O}(1) \]

				\begin{equation*}
					\textup{I:}
		  		\left[
		  		\begin{array}{l}
		    		\textup{Recorre 2 veces, un camino de longitud } d = d(t)
		  		\end{array}
		  		\right.
				\end{equation*}

				\[ \Rightarrow \textup{ I es } \mathcal{O}(d) \]

				\par Por lo tanto:
				\begin{eqnarray}
					\nonumber Compl(\underbrace{A \; \dotsc \; A}_{j \; veces}R) &=& \underbrace{\mathcal{O}(1) + \; \dotsc \; \mathcal{O}(1)}_{j \; veces} + \mathcal{O}(1) \\
					\nonumber &=& \mathcal{O}(j) + \mathcal{O}(1) \\
					\nonumber &=& \mathcal{O}(j)
				\end{eqnarray}

				\par Pero como cada A hace $ i = i + 1$ y tenemos $ 0 \leq \textit{i} \leq \textit{d} \; \Rightarrow \; \textit{j} \leq \textit{d}.$

				\[ \therefore \textup{ Compl(A } \dotsc \textup{ AR } = \mathcal{O}(d) \]

				\par Similarmente:
				\begin{eqnarray}
					\nonumber Compl(A \; \dotsc \; AI) &=& \underbrace{\mathcal{O}(1) + \; \dotsc \; \mathcal{O}(1)}_{\leq \; d \; veces} + \mathcal{O}(1) \\
					\nonumber &=& \mathcal{O}(d) + \mathcal{O}(1) \\
					\nonumber &=& \mathcal{O}(d)
				\end{eqnarray}

			\textbf{Cantidad de subpalabras}
				\begin{itemize}
					\item R tiene la instrucción "\textbf{borrar lado}". Como los lados borrados quedan borrados hay a lo sumo \textit{m} R's, es decir:
					\[ \therefore \textup{ A } \dotsc \textup{ AR's } \leq m \]
					\item I tiene también línes de la forma:
						\textit{if} ($\dotsc$) \textit{then} Borrar lado
					 	\par Lo que está dentro del \textbf{if} se cumple al menos una vez, es decir:
						\[ \therefore \textup{ (A } \dotsc \textup{ AI's }) \leq m \]

					\par Este análisis muestra que:
					\[ \therefore \# \textup{ (A } \dotsc \textup{ AI's }) + \# \textup{ (A } \dotsc \textup{ AR's }) \leq m \]
					\par Por lo tanto hay $\leq m$ palabras, cada una de complejidad $\mathcal{O}(d)$.
					\begin{eqnarray}
						\nonumber \therefore \;  Compl(Paso \; Bloqueante) &=& \mathcal{O}(m) + \mathcal{O}(md) \\
						\nonumber &=& \mathcal{O}(md)
				\end{eqnarray}
				\par Como $d \leq n$, queda entonces demostrado.
				\end{itemize}


	\section{La complejidad de WAVE}
		\textbf{\underline{Teorema:}} La complejidad del algoritmo de Wave es $\mathcal{O}(n^{3})$.

		\textbf{\underline{Prueba:}} Como Wave es un algoritmo que trabaja con networks auxiliares y vimos que la distancia entre \textit{s} y \textit{t} en networks auxiliares consecutivos aumenta y puede ir a lo sumo entre 1 y $n - 1$ entonces hay a lo sumo $\mathcal{O}(n)$ networks auxiliares.
			\begin{center}
				Complejidad(Wave)$ = \mathcal{O}(n) *$ Compl(Paso bloqueante de Wave)
			\end{center}

			\par Para probar que la complejidad de Wave es $\mathcal{O}(n^{3})$ debemos probar que complejidad del paso bloqueante es $\mathcal{O}(n^{2})$. El paso bloqueante de Wave consiste en una serie de:
			\begin{itemize}
				\item Olas hacia adelante: Sucesión de \textbf{fordwardbalance} (FB)
				\item Olas hacia atrás: Sucesión de \textbf{backwardbalance} (BB)
			\end{itemize}

			\par Cada FB y BB es una sucesión de \textquotedblleft \textbf{buscar vecinos}\textquotedblright y \textquotedblleft \textbf{procesar}\textquotedblright el lado resultante. Estos "procesamientos" \; son complicados pero $ \mathcal{O}(1)$.
			\[ \therefore \textup{ Compl(Paso Bloqueante) } = \# \textup{ \textquotedblleft procesamientos de lado\textquotedblright} \]

			\par Los "procesamientos" de lados los podemos dividir en dos categorías:
			\begin{enumerate}
				\item Aquellos procesamientos que saturan o vacian el lado. Denotaremos \textquotedblleft T\textquotedblright \; al número de estos procesamientos.
				\item Aquellos procesamientos que no saturan ni vacian el lado. Denotaremos \textquotedblleft Q\textquotedblright \; al número de estos procesamientos.
			\end{enumerate}
			\par Por lo tanto queremos acotar $T + Q$.

			\vspace{5mm}
			\textbf{Complejidad de T:}
			\begin{itemize}
				\item ¿Puede un lado $\overrightarrow{xy}$ saturado volver a saturarse?
					\par Para poder volver a \underline{saturarse} primero tiene que vaciarse auque sea un poco, es decir, antes de poder volver a saturarlo \textquotedblleft \textit{y}\textquotedblright \; debe devolver flujo a \textquotedblleft \textit{x}\textquotedblright \;, pero para que en Wave \textquotedblleft \textit{y}\textquotedblright \; le devuelva flujo a \textquotedblleft \textit{x}\textquotedblright \;  debe ocurrir que \textquotedblleft \textit{y}\textquotedblright \; este bloqueado (porque BB(y) solo se ejecuta si \textquotedblleft \textit{y}\textquotedblright \; está bloqueado), pero si \textquotedblleft \textit{y}\textquotedblright \; está bloqueado \textquotedblleft \textit{x}\textquotedblright \; no puede mandarle flujo nunca más.
					\[ \therefore \overrightarrow{xy} \textup{ no puede resaturarse } \]
					\par \underline{Conclusión 1:} Los lados se saturan solo una vez.
				\item ¿Puede un lado $\overrightarrow{xy}$ vaciado completamente volver a vaciarse?
					\par Para poder volver a \underline{vaciarse} como está vacío completamente, primero hay que mandar flujo, pero si lo vacié \textquotedblleft \textit{y}\textquotedblright \; está bloqueado por lo que \textquotedblleft \textit{x}\textquotedblright \; no puede mandar flujo.
					\[ \therefore \overrightarrow{xy} \textup{ no puede volver a vaciarse } \]
					\par \underline{Conclusión 2:} Los lados se vacían completamente a lo sumo una vez.

				\par Las conclusiones (1) y (2) implican que \begin{tabular}{|c|} \hline $T \leq 2 \; m$ \\\hline \end{tabular}
			\end{itemize}

			\textbf{Complejidad de Q:}
				\par En cada FB a lo sumo un lado no se satura y en cada BB a lo sumo un lado no se vacía completamente.
				\[ \therefore Q \leq \# \textup{ Total de FB's y BB's} \]
				\begin{itemize}
					\item \# FB's en cada ola hacia adelante es $\leq$ \textit{n} (un FB por vértice)
					\item \# BB's en cada ola hacia atrás es $\leq$ \textit{n}

					\vspace{5mm}
					\par $\therefore$ Total de FB's y BB's $\leq 2 n \; \#$Total de ciclos de \textquotedblleft ola adelante $-$ ola hacia atrás\textquotedblright
	 			\end{itemize}

				\par Ahora bien, en cada ola hacia adelante, pueden o no, bloquearse algunos vértices. Si no se bloquea ningún vértice entonces todos los vértices ($\neq$ \textit{s, t}) quedan balaceados por lo que estamos en la última ola. Luego en toda ola que no sea la última se bloquea al menos un vértice ($\neq$ \textit{s, t}).
				\begin{center}
					$\therefore \; \#$ Total de ciclos es $\leq (n - 2) + 1 = n - 1$ \\
					\vspace{3mm}
					$\Rightarrow$ \begin{tabular}{|c|} \hline $Q \; \leq 2 n \; (n - 1) = \mathcal{O}(n^{2})$ \\ \hline \end{tabular}
				\end{center}

				\begin{eqnarray}
					\nonumber \therefore \; T + Q &\leq & 2 m + \mathcal{O}(n^{2}) \\
					\nonumber &=& \mathcal{O}(m) + \mathcal{O}(n^{2}) \\
					\nonumber &=& \mathcal{O}(n^{2})
				\end{eqnarray}


	\section{La distancia entre NA sucesivos aumenta}
		\textbf{\underline{Teorema:}} Sea A un NA (network auxiliar) y sea $A^{*}$ el siguiente NA. Sean d(x) y $d^{*}(x)$ las distancias de \textit{s} a \textit{t} en A y $A^{*}$ respectivamente, entonces: $d(t) < d^{*}(t)$.

		\textbf{\underline{Prueba:}} Como A y $A^{*}$ se construyen con BFS sabemos que $d(t) \leq d^{*}(t)$ pero queremos ver el $<$.
			\par Sea:
			\[ s = x_{0}, x_{1}, \dotsc t = x_{r} \]
			\par un camino dirigido en $A^{*}$.
			\par Ese camino \begin{tabular}{|c|} \hline No existe \\\hline \end{tabular} en A ya que para pasar de A a $A^{*}$ debemos bloquear todos los caminos dirigidos de A. Por lo tanto si ese camino estuviese en A, Dinic lo habría bloqueado y no estaría en $A^{*}$.

			\vspace{5mm}
			\textbf{¿Cuáles son las razones posibles para que no esté en A?}
				\begin{enumerate}
					\item Puede faltar un vértice, es decir $\exists i : x_{i} \; \nexists \; V(A)$ entonces:
						\begin{eqnarray}
							\nonumber d(t) &\leq & d(x_{i}) \\
							\nonumber &\leq & d^{*}(x_{i}) \; \; \textup { Por } \langle E-K \rangle \\
							\nonumber &=& d^{*}(t) \; \; \; \; \textup{ Porque } x_{i} \textup{ esta antes que t}
						\end{eqnarray}
						$\qquad\qquad\qquad\qquad\;$
						\begin{tabular}{|c|} \hline $\therefore \; d(t) < d^{*}(t)$ \\\hline \end{tabular}
					\item Están todos los vértices pero falta una arista, es decir $\exists i : \overrightarrow{x_{i}x_{i + 1}} \; \notin E(A)$.
						\begin{enumerate}[a)]
							\item $\overrightarrow{x_{i}x_{i + 1}}$ no está porque corresponde a un lado vacío o saturado en NA, es decir $\overrightarrow{x_{i}x_{i + 1}}$ no está en el residual que dá origen a A pero si está en el residual que dá origen a $A^{*}$.
								\par Para que esto pase se tiene que haber usado el lado $x_{i + 1}x_{i}$ en A. Luego podemos concluir, por la prueba de $\langle E-K\rangle$ que:
								\[ d^{*}(t) \geq d(t) + 2 > d(t) \]
								$\qquad\qquad\qquad\qquad\qquad\qquad$
								\begin{tabular}{|c|} \hline $\therefore \; d(t) < d^{*}(t)$ \\\hline \end{tabular}
							\item $\overrightarrow{x_{i}x_{i + 1}}$ si está en el residual pero:
								\begin{tabular}{|c|} \hline $d(x_{i+1}) \neq d(x_{i}) +1 $ \\ \hline \end{tabular} (1)

								\vspace{5mm}
								\par Pero como $x_{i}x_{i + 1}$ está en el residual entonces:
								\begin{tabular}{|c|} \hline $d(x_{i+1}) \leq d(x_{i}) +1 $ \\ \hline \end{tabular} (2)

								\vspace{5mm}
								\par De (1) y (2) tenemos que: \begin{tabular}{|c|} \hline $d(x_{i+1}) < d(x_{i}) +1 $ \\ \hline \end{tabular} $\circledast$

								\vspace{3mm}
								\par Entonces:
								\begin{eqnarray}
									\nonumber d(t) &=& d(x_{i + 1}) + b(x_{i + 1}) \qquad\;\;\;\;\; \textup{Por } \langle E-K\rangle \\
									\nonumber &\leq & d(x_{i + 1}) + b^{*}(x_{i + 1}) \qquad\;\;\;\; \textup{Por } \langle E-K\rangle \\
									\nonumber &<& d(x_{i}) + 1 + b^{*}(x_{i + 1}) \qquad\; \textup{Por } \circledast \\
									\nonumber &\leq & d^{*}(x_{i}) + 1 + b^{*}(x_{i + 1}) \qquad \textup{Por } \langle E-K\rangle \\
									\nonumber &=& d^{*}(x_{i + 1}) + b^{*}(x_{i + 1}) \qquad\;\;\; \textup{Por } (\dag) \\
									\nonumber &=& d^{*}(t)
								\end{eqnarray}
								$\qquad\qquad\qquad\;\;\;$
								\begin{tabular}{|c|} \hline $\therefore \; d(t) < d^{*}(t)$ \\ \hline \end{tabular}

								\vspace{5mm}
								\par ($\dag$): Ya que $s, \; x_{1}, \; \dotsc \; x_{r}$ es un camino en A*.
						\end{enumerate}
			\end{enumerate}



\chapter{Parte B}

	\section{2-COLOR es polinomial}
		\textbf{\underline{Teorema:}} 2-Color es polinomial, es decir, existe un algoritmo polinomial que lo resuelve.

		\textbf{\underline{Prueba:}} Consideremos el siguiente algoritmo con entrada $G = (V, E)$ con $n = \abs{V}$ y $m = \abs{E}$.

			\vspace{5mm}
			\par \textbf{Algoritmo:}
			\par Por cada COMPONENTE CONEXA:
				\begin{enumerate}
					\item Elegir x
					\item Correr BFS(x)
					\item Colorear vertices con: $\qquad	C(v) = Nivel_{BFS(x)}(v) \; mod \; 2$
					\item Chequear que el coloreo de (3) sea propio. Si lo es, RETURN \textquotedblleft SI\textquotedblright, si no lo es, RETURN \textquotedblleft NO\textquotedblright.
				\end{enumerate}

			\par Veamos ahora que el algoritmo es correcto y que su complejidad es polinomial.
			\begin{itemize}
				\item \underline{Complejidad Polinomial:}
					\begin{itemize}
						\item (1) es $\mathcal{O}(1)$.
						\item (2) + (3) es $\mathcal{O}(m)$ (recorrer todos los vértices).
						\item (4) es $\mathcal{O}(m)$ ya que chequear que un coloreo es propio, es recorrer todos los lados comprobando que los vértices tengan distintos colores.
					\end{itemize}
				\item \underline{Correctitud:}
					\begin{itemize}
						\item Cuando dice \textquotedblleft SI\textquotedblright: Obviamente dice \textquotedblleft SI\textquotedblright, solo si el coloreo es propio y si el coloreo es propio, $\chi(G) \leq 2$.
						\item Cuando dice \textquotedblleft NO\textquotedblright: Tenemos que ver que cuando dice \textquotedblleft NO\textquotedblright ningún coloreo propio con 2 colores existe.

							\vspace{5mm}
							\par Por otro teorema visto en clase, si existe un ciclo impar $C_{2k + 1} \subseteq G \Rightarrow \chi(G) \geq 3$. Por lo que bastaría ver que si el algoritmo dice \textquotedblleft NO\textquotedblright, entonces exista un ciclo impar en G.

							\vspace{5mm}
							\par Si el algoritmo dice \textquotedblleft NO\textquotedblright $\Rightarrow \exists z, y : zy \in E(G) \textup{ y } C(z) = C(y) \Rightarrow Nivel_{BFS(x)}(z) = Nivel_{BFS(x)}(y)$

							\vspace{5mm}
							\par Sea:
							\begin{itemize}
								\item $x \dotsc z$ un camino en BFS(x), entre x y z.
								\item $t = Nivel_{BFS(x)}(z)$.
								\item $x \dotsc y$ un camino en BFS(x), entre x e y.
								\item $r = Nivel_{BFS(x)}(y)$.
								\item \textit{w} el vértice de mayor nivel tal que: $x \dotsc w$ sea prefijo común en $x \dotsc z$ y $x \dotsc y$. Es decir:
									\begin{eqnarray}
										\nonumber &x \dotsc w & \qquad \dotsc z \\
										\nonumber &\underbrace{x \dotsc w}_{\textup{Camino Igual}}& \; \underbrace{\dotsc y}_{\textup{Camino distinto}}
									\end{eqnarray}
							\end{itemize}

						\par Considramos el ciclo: $w \dotsc z y \dotsc w$
						\begin{eqnarray}
							\nonumber \textup{Longitud del ciclo } &=& (t - s) + 1 + (r - s) \\
							\nonumber &=& (t+r) - 2s + 1 \\
							\nonumber &\equiv & 0 + 0 + 1 = 1 \; (2) \\
							\nonumber \therefore \textup{Es un ciclo impar}
						\end{eqnarray}
					\end{itemize}
			\end{itemize}


	\section{Teorema Max-Flow Min-Cut}
		\textbf{\underline{Teorema:}}
			\begin{enumerate}[a)]
				\item Si \textit{f} es flujo y S es corte $\Rightarrow$ V(\textit{f}) $\leq$ Cap(S).
				\item Si V(\textit{f}) $=$ Cap(S) $\Rightarrow$ \textit{f} es maximal y S es minimal.
				\item Si \textit{f} es maximal $\Rightarrow \exists$ S con V(\textit{f}) $=$ Cap(S).
			\end{enumerate}

		\textbf{\underline{Prueba:}} Denotemos $(\star)$, a la demostración de:
			\begin{center}
				\begin{tabular}{|c|} \hline $V(\textit{f}) = f(S, \overline{S}) - f(\overline{S},S)$ \\\hline \end{tabular}
			\end{center}

			\vspace{5mm}
			\textbf{a) \textit{f} es flujo y S es corte $\Rightarrow$ V(\textit{f}) $\leq$ Cap(S).}
				\begin{eqnarray}
					\nonumber V(f) &\stackbin{Por \; (\star)}{=}& f(S, \overline{S})\underbrace{-\underbrace{f(\overline{S}, S)}_{\geq 0}}_{\leq 0} \\
					\nonumber &\leq & f(S, \overline{S}) \\
					\nonumber &\leq & Cap(S, \overline{S}) \\
					\nonumber &=& Cap(S)
				\end{eqnarray}


			\textbf{b) V(\textit{f}) $=$ Cap(S) $\Rightarrow$ \textit{f} es maximal y S es minimal.}

				\vspace{2mm}
				\par Supongamos que V(\textit{f}) $=$ Cap(S). Sea \textit{g} un flujo cualquiera y T un corte cualquiera.
				\begin{itemize}
					\item $V(g) \stackbin{Por \; a)}{\leq} Cap(S) = V(f) \Rightarrow$ f es maximal.
					\item $Cap(T) \stackbin{Por \; a)}{\geq} V(f) = Cap(S) \Rightarrow$ S es minimal.
				\end{itemize}

			\vspace{5mm}
			\textbf{c) \textit{f} es maximal $\Rightarrow \exists$ S con V(\textit{f}) $=$ Cap(S).}

				\vspace{2mm}
				\par Sea $S = \{s\} \cup \{x : \exists \;$ camino aumentante realtivo a \textit{f} entre \textit{s} y \textit{x}$\}$

				\vspace{5mm}
				\begin{tabular}{|c|} \hline ¿$t \in S$? \\\hline \end{tabular}
				¿$t \in S$?

					\vspace{3mm}
					\underline{Si \textit{t} estuviera en S:} existiría un camino aumentante entre \textit{s} y \textit{t}.
					\par Por el teorema del camino aumentante podemos construir un flujo \textit{g} tal que:

					\begin{eqnarray}
						\nonumber V(g) &=& V(f) + \epsilon \textup{ para algun } \epsilon > 0 \\
						\nonumber \therefore V(g) &>& V(f) \textup{ Absurdo! pues f es maximal} \\
						\nonumber \therefore t &\notin & S \Rightarrow \textup{ S es corte.}
					\end{eqnarray}
					\[ \textup{Solo resta ver que: } V(f) = Cap(S) \]
					\par Por $(\star): \; V(f) =  \underbrace{f(S, \overline{S})}_{(1)} - \underbrace{f(\overline{S}, S)}_{(2)}$

					\vspace{2mm}
					\par Analicemos (1) y (2)
					\begin{enumerate}[(1)]
						\item $f(S, \overline{S}) = \sum_{\begin{subarray}{l} \; x \in S \\
							\; y \in \overline{S} \\ xy \in E\end{subarray}} f(\overrightarrow{xy}) $
							\par $x \in S \Rightarrow \exists$ camino aumentante $ s, \dotsc x$.
							\par $y \in \overline{S} \Rightarrow \nexists$ camino aumentante entre \textit{s} y \textit{y}. En particular $s, \dotsc x, \dotsc y$ no es camino aumentante, por lo que no puede darse que: $f(\overrightarrow{xy}) < Cap(\overrightarrow{xy})$.

							\vspace{3mm}
							\par $\Rightarrow f(\overrightarrow{xy}) = Cap(\overrightarrow{xy}) \qquad \forall x \in S, \forall y \in \overline{S} : \overrightarrow{xy} \in E.$

							\vspace{3mm}
							\par $\Rightarrow$ \begin{tabular}{|c|} \hline $f(S, \overline{S})$ \\\hline \end{tabular} $= \sum_{\begin{subarray}{l} \; x \in S\\  \; y \in \overline{S} \\ xy \in E\end{subarray}} f(\overrightarrow{xy}) = \sum_{\begin{subarray}{l} \; x \in S\\  \; y \in \overline{S} \\ xy \in E\end{subarray}} Cap(\overrightarrow{xy}) = Cap(S, \overline{S}) =$ \begin{tabular}{|c|} \hline $ Cap(S) $ \\ \hline \end{tabular}

						\item $f(\overline{S}, S) = \sum_{\begin{subarray}{l} \; x \in \overline{S} \\
							\; y \in S \\ xy \in E\end{subarray}} f(\overrightarrow{xy}) $
							\par $x \in \overline{S} \Rightarrow \nexists$ camino aumentante entre \textit{s} y \textit{x}.
							\par $y \in S \Rightarrow \exists$ camino aumentante $ s, \dotsc y$.
							\par En particular $s \dotsc y, y \dotsc x$ no es camino aumentante $\Rightarrow f(\overrightarrow{xy}) = 0  \; \forall x \in \overline{S}, \forall y \in S : \overrightarrow{xy} \in E$.

							\vspace{3mm}
							\par $\therefore$ \begin{tabular}{|c|} \hline $ f(\overline{S}, S) = 0 $ \\\hline \end{tabular}
					\end{enumerate}

					\par Luego de (1) y (2):
					\begin{eqnarray}
						\nonumber V(f) &=& f(S, \overline{S}) - f(\overline{S}, S)\\
						\nonumber &=& Cap(S) - 0 \\
						\nonumber &=& Cap(S)
					\end{eqnarray}


	\section{Complejidad del Hungaro es $\mathcal{O}(n^{4})$}
		\textbf{\underline{Teorema:}} La complejidad del algoritmo Húngaro es $\mathcal{O}(n^{4})$.

		\textbf{\underline{Prueba:}}
			\begin{enumerate}
				\item La complejidad del matching inicial es $\mathcal{O}(n^{2})$, ya que:
					\par Restar mínimo de cada fila:
					\begin{center}
						$(\underbrace{\mathcal{O}(n)}_{calcular \; min} + \underbrace{\mathcal{O}(n)}_{restar \; min}) * \underbrace{n}_{\# filas} = \mathcal{O}(n^{2})$
					\end{center}
					\par Idem para las columnas.
				\item Llamemos \textbf{extender} el matching, a incrementar su número de filas en 1, es decir, agregar una fila más al matching.
					\begin{center}
						$\#$ extensiones de matching $= \mathcal{O}(n)$
					\end{center}

					\par Resta ver la complejidad de cada \textbf{extender}.
				\item En cada extensión vamos a ir revisando filas y columnas, donde escanear una fila es $\mathcal{O}(n)$ y se realizan \textit{n} escaneos, por lo que sería $\mathcal{O}(n^{2})$ sin considerar que se debe realizar un cambio de matriz.
					\par Hacer un \textbf{cambio de matriz} es $\mathcal{O}(n^{2})$, ya que:
					\begin{itemize}
						\item Buscar $\textit{m} = \min S$ x $\overline{\Gamma(S)} \rightarrow \mathcal{O}(n^{2})$
						\item Restar \textit{m} de $S \rightarrow \mathcal{O}(n^{2})$
						\item Sumar \textit{m} a $\Gamma(S) \rightarrow \mathcal{O}(n^{2})$
					\end{itemize}

					\par Luego la implementación NAIVE lanzaría nuevamente el algoritmo desde cero. La forma correcta es continuar con el matching que teniamos, ya que el mismo no se pierde.
					\par Si lo hacems así, ¿Cuántos \textbf{cambios de matriz} habrá antes de extender un matching nuevamente?

					\underline{\textbf{Lema Interno:}} Luego de un \textbf{cambio de matriz}, se extiende el matching o bien se aumenta S.
						\par \underline{\textbf{Prueba:}}
						\begin{center}$
							\; \; \left(
							\begin{array}{r | r}
								\text{{\huge A}} & \text{{\huge A}} \\
								\hline
								\text{{\huge B}} & \text{{\huge C}}
							\end{array}
							\right)
							{\begin{subarray}{l} \; \overline{\text{{\large S}}} \\ \\ \; {\text{{\large S}}} \end{subarray}} \linebreak
							\Gamma(S) \; \overline{\Gamma(S)}
							$
						\end{center}

						\underline{Referencias:}
						\begin{itemize}
							\item A: puede haber ceros.
							\item B: ceros del matching.
							\item C: no hay ceros, no hay matching.
						\end{itemize}

						\par Al restar $\textit{m} = \min S$ x $\overline{\Gamma(S)}$ de las filas de S, habrá un nuevo cero en alguna fila \textit{i} ($\in S$) y columna \textit{j} ($\in \overline{\Gamma(S)}$) entonces la columna se etiquetará con \textit{i} y se revisará.
						Tenemos dos resultados posible:
						\begin{enumerate}
							\item \textit{j} está libre (i.e no forma parte del matching) $\Rightarrow$ extendemos el matching.
							\item \textit{j} forma parte de matching $\Rightarrow \exists$ fila \textit{k} matcheada con \textit{j}. En este caso, la fila \textit{k} se etiquetará con \textit{j}, por lo que el "nuevo" $\; S \geq S \cup \{\textit{k}\}$.

							\vspace{5mm}
							\par Entonces se termina con una extensión o se produce un nuevo S de cardinalidad, al menos $\lvert S \rvert + 1$.
						\end{enumerate}
					\textbf{Fin Lema Interno}

					\par Luego como $\lvert S \rvert$ solo puede crecer $\mathcal{O}(n)$ veces, tenemos que hay a lo sumo \textit{n} \textbf{cambios de matriz} antes de extender el matching. Entonces:
						\begin{center}
							\par Complejidad(1 Extensión) $= \underbrace{\mathcal{O}(n)}_{\# CM} * \underbrace{\mathcal{O}(n^{2})}_{Compl(CM)} + \underbrace{\mathcal{O}(n^{2})}_{Busqueda \; \textit{n} \; filas \; x \; \textit{n} \; columnas }$ \\

							\vspace{5mm}
							\par $\therefore$ Complejidad(Húngaro) $= \underbrace{\mathcal{O}(n^{2})}_{Matching \; inicial} + \; (\underbrace{\mathcal{O}(n)}_{\#extensiones} * \underbrace{\mathcal{O}(n^{3})}_{Compl(extension)}) = \; \mathcal{O}(n^{4})$
						\end{center}
			\end{enumerate}


	\section{Teorema de Hall}
		\textbf{\underline{Teorema:}} Sea G = (X $\cup$ Y, E) grafo bipartito $\Rightarrow \exists$ matching completo de X a Y $\Leftrightarrow \; \lvert S \rvert \leq \lvert \Gamma(S) \rvert \; \forall S \subseteq X$.

		\vspace{3mm}
		\textbf{\underline{Prueba:}}
			\par $\Rightarrow)$ Si M es matching completo de X a Y entonces observemos que M induce una función inyectiva de X a Y.
			\[ f(x) = \textup{ unico } y : xy \in E(M) \]

			\begin{enumerate}
				\item Si $S \subseteq X \Rightarrow \lvert S \rvert = \lvert f(S) \rvert$
				\item Además por definición de f, $f(x) \in \Gamma(x)$
					\begin{eqnarray}
						\nonumber \text{Si x} \in S &\Rightarrow & f(x) \in \Gamma(S) \\
						\nonumber &\Rightarrow & f(S) \subseteq \Gamma(S) \\
						\nonumber &\therefore & \lvert f(S) \rvert \leq \lvert \Gamma(S) \rvert
					\end{eqnarray}
			\end{enumerate}
			\par De (1) y (2) $\Rightarrow \lvert S \rvert \leq \lvert \Gamma(S) \rvert$.

			\vspace{5mm}
			\par $\Leftarrow)$ Supongamos que no es cierto, entonces G es bipartito con $\lvert S \rvert \leq \lvert \Gamma(S) \rvert \; \forall S \subseteq X$ pero no tiene matching completo de X a Y. Esto equivalente a ver que: si $\nexists$ un matching completo de X a Y $\Rightarrow \exists \; S \subseteq X : \lvert S \rvert > \lvert \Gamma(S) \rvert$.

			\vspace{5 mm}
			\par Corramos el algoritmo para hallar matching. Al finalizar habrá filas sin matcher (las de \textit{s}).

			\vspace{5mm}
			\par Sean:
			\begin{itemize}
				\item $S_{0} = \lbrace \textup{filas sin matchear} \rbrace$.
				\item $T_{1} = \Gamma(S_{0})$, es decir, las columnas etiquetadas por las filas de $S_{0}$. Todas las columnas de $T_{1}$ están matcheadas, pues si no, se podría agregar alguna alguna fila de $S_{0}$ al matching.
				\item $S_{1} = \lbrace \textup{filas etiquetadas por las columnas de } T_{1}\rbrace$.
				\item $T_{2} = \Gamma(S_{1}) - T_{1}$, es decir, columnas etiquetadas por las filas de $S_{1}$.
			\end{itemize}

			\par En general:
			\begin{center}
				\begin{itemize}
					\item $S_{i} = \lbrace \textup{filas matcheadas con } T_{i}\rbrace$.
					\item $T_{i+1} = \Gamma(S_{i}) - (T_{1} \cup T_{2} \cup \dotsc T_{i})$.
				\end{itemize}
			\end{center}

			\par Como el algoritmo para sin hayar matching, entonces $\forall i \; T_{i} \neq \emptyset $, produce un $S_{i}$  (i.e $S_{i} \neq \emptyset$).
			\par $\therefore$ La única forma de parar es en un \textit{k}, tal que $T_{k+1} = \emptyset$.

			\vspace{5mm}
			\underline{Observaciones:}
			\begin{enumerate}
				\item $\lvert S_{j} \rvert = \lvert T_{j} \rvert$, pues $S_{j}$ son las filas matcheadas con $T_{j}$.
				\item Por construcción, los $S_{i}$ y $T_{i}$ son todos distintos.
				\item $\Gamma(S_{0} \cup S_{1} \cup \dotsc S_{j}) = T_{1} \cup T_{2} \cup \dotsc T_{j+1}$
					\par \underline{Por inducción en \textit{j}:}
						\begin{itemize}
							\item Caso Base: $j = 0$ vale, ya que $T_{1} = \Gamma(S_{0})$
							\item Caso Inductivo: Supongamos que vale para \textit{j}, veamos para \textit{j+1}.
								\begin{eqnarray}
									\nonumber T_{1} \cup T_{2} \dotsc T_{j + 1} &=& T_{1} \cup T_{2} \cup \dotsc T_{j + 1} \cup \underbrace{(\Gamma(S_{j + 1}) - (T_{1} \cup T_{2} \dotsc T_{j + 1}))}_{T_{j + 2}} \\
									\nonumber &=& T_{1} \cup T_{2} \cup \dotsc T_{j + 1} \cup (\Gamma(S_{j + 1}) \\
									\nonumber &=& \Gamma(S_{0} \cup S_{1} \cup \dotsc S_{j}) \cup \Gamma(S_{j + 1}) \qquad \text{Por H.I} \\
									\nonumber &=& \Gamma(S_{0} \cup S_{1} \cup \dotsc S_{j} \cup S_{j + 1})
								\end{eqnarray}
						\end{itemize}
			\end{enumerate}

			\par Sea $S = S_{0} \cup S_{1} \cup \dotsc S_{k}$
				\begin{eqnarray}
					\nonumber \lvert \Gamma(S) \rvert &=& \lvert \Gamma(S_{0} \cup S_{1} \cup \dotsc S_{k}) \rvert \\
					\nonumber &=& \lvert T_{1} \cup T_{2} \cup \dotsc \underbrace{T_{k+1}}_{= \; \emptyset} \rvert \qquad \; \text{Por Obs 3} \\
					\nonumber &=& \lvert T_{1} \cup T_{2} \cup \dotsc T_{k} \rvert \\
					\nonumber &=& \lvert T_{1} \rvert + \lvert T_{2} \rvert + \dotsc \lvert T_{k} \rvert \qquad \text{Por Obs 2} \\
					\nonumber &=& \lvert S_{1} \rvert + \lvert S_{2} \rvert + \dotsc \lvert S_{k} \rvert \qquad \text{Por Obs 1} \\
					\nonumber &=& \lvert S \rvert - \lvert S_{0} \rvert \qquad \qquad \qquad \; \; \text{Por Obs 2} \\
					\nonumber &<& \lvert S \rvert \qquad \qquad \qquad \qquad \; \; \; \; \;\text{Pues } S_{0} \neq \emptyset \\
					\nonumber & \text{Absurdo!} &
				\end{eqnarray}

			\par El absurdo vino de suponer que G es bipartito con $\lvert S \rvert \leq \lvert \Gamma(S) \rvert \; \forall S \subseteq X$ pero que no tiene matching completo de X a Y.


	\section{Teorema del matrimonio}
		\textbf{\underline{Teorema:}} Todo grafo regular bipartito tiene un matching perfecto.

		\textbf{\underline{Prueba:}} Sea $G = (X \cup Y, E)$ bipartito regular, tal que $\forall W \subseteq V(G)$, definimos:
			\begin{eqnarray}
				\nonumber E_{W} &=& \left\lbrace xy \in E(G): x \in W \; o \ y \in W \right\rbrace \\
				\nonumber &=& \lbrace \text{lados con un extremo en W} \rbrace
			\end{eqnarray}

			\par Supongamos que $W \subseteq X$ (de igual forma para $W \subseteq Y$). Además, como G es regular, $\exists \; \Delta  = \delta > 0 : d(z) = \Delta \; \; \forall z$.
			\begin{eqnarray}
				\nonumber \lvert E_{w} \rvert &=& \lvert \left\lbrace xy \in E: x \in W \right\rbrace \rvert \\
				\nonumber &=& \sum_{x \in w} \lvert \left\lbrace y : xy \in E \right\rbrace \rvert \\
				\nonumber &=& \sum_{x \in w} \; \underbrace{d(x)}_{\Delta} \\
				\nonumber &=& \Delta * \lvert w \rvert
			\end{eqnarray}

			\par Es decir, a cada \textit{w} le corresponden $\Delta$ lados distintos. En particular:
			\begin{eqnarray}
				\nonumber \lvert E_{x} \rvert &=& \Delta * \lvert x \rvert \\
				\nonumber \lvert E_{y} \rvert &=& \Delta * \lvert y \rvert
			\end{eqnarray}
			\par pero $E_{x} = E = E_{y}$ pues G es bipartito.

			\vspace{5mm}
			\par Por lo tanto:
			\begin{eqnarray}
				\nonumber \lvert E_{x} \rvert = \lvert E_{y} \rvert &\Rightarrow & \Delta * \lvert x \rvert = \Delta * \lvert y \rvert \\
				\nonumber &\Rightarrow & \lvert x \rvert = \lvert y \rvert \\
				\nonumber &\Rightarrow & \textup{ Todo matching completo es perfecto}
			\end{eqnarray}

			\vspace{3mm}
			\par Basta ver que existe un matching completo de X a Y, es decir, que se cumple la condición de Hall.

			\vspace{5mm}
			\par Sea $S \subseteq X$:

			\begin{equation*}
				\text{Sea}
				\underbrace{xy}_{\begin{subarray}{l} \; x \in X \\
				\; y \in Y \end{subarray}} \in E_{s} \Rightarrow
				{\Huge \exists}
	  		\left\lbrace
	  		\begin{array}{l}
	    			x \in S \\
	     		  y \in \Gamma(x) \\
	  		\end{array}
	 			\right\rbrace
	 			\Rightarrow y \in \Gamma(S) \Rightarrow xy \in E_{\Gamma(s)}
			\end{equation*}

			\vspace{3mm}
			\par Es decir, hemos probado que:

			\begin{eqnarray}
				\nonumber E_{s} &\subseteq & E_{\Gamma(S)} \\
				\nonumber \lvert E_{S} \rvert &\leq & \lvert E_{\Gamma(S)} \rvert \\
				\nonumber \Delta \; \lvert S \rvert &\leq & \Delta \; \lvert \Gamma(S) \rvert \\
				\nonumber \lvert S \rvert &\leq & \Gamma(S)
			\end{eqnarray}


	\section{Todo grafo bipartito es $\Delta$ coloreable}
		\textbf{\underline{Teorema:}} Si G es bipartito $\Rightarrow \chi '(G) = \Delta $

		\textbf{\underline{Prueba:}}

			\vspace{3mm}
			\textbf{Lema Interno:} Todo grafo bipartito es subgrafo de un grafo bipartito regular con el mismo $\Delta$, es decir:
				\begin{center}
					G bipartito $\Rightarrow \exists$ H bipartito regular con G $\subseteq$ H y $\Delta(G) = \Delta(H)$
				\end{center}

				\underline{Prueba:} Sean:
					\begin{itemize}
						\item $G = (V = X \cup Y, E)$ grafo bipartito
						\item $G^{\star} = (V^{\star}, E^{\star})$ una copia de G
						\item $E^{\dag} = \left\lbrace vv^{\star} : d_{G} < \Delta(G) \right\rbrace$
						\item $\overline{G} = (\overline{V}, \overline{E})$ con:
							\begin{itemize}
								\item $\overline{V} = V \cup V^{\star}$
								\item $\overline{E} = E \cup E^{\star} \cup E^{\dag}$
							\end{itemize}
					\end{itemize}

					\par Propiedades de $\overline{G}$:
					\begin{enumerate}
						\item $\overline{G}$ es bipartito, sus partes son:
							\begin{itemize}
								\item $X \cup Y^{\star}$
								\item $X^{\dag} \cup Y$
							\end{itemize}
							\par No existen lados entre $x \leftrightarrow x, \; y^{\star} \leftrightarrow y^{\star}, \; x \leftrightarrow y^{\star}$.

						\item Sea v $\in$ V tal que $d_{G}(v) = \Delta = \Delta(G)$ entonces V no es parte de ningún lado de $E^{\dag}$.
							\par $\Rightarrow d_{\overline{G}}(v) = d_{\overline{G}}(v^{\star}) = \Delta$

							\vspace{5mm}
							\par Sea v $\in$ V tal que $d_{G}(v) < \Delta \Rightarrow$ forma parte de un nuevo lado.
							\par $\therefore d_{\overline{G}}(v) = d_{\overline{G}}(v^{\star)} = d_{G}(v) + 1$

							\vspace{5mm}
							\par \underline{Conclusión:}
							\begin{eqnarray}
								\nonumber \Delta(\overline{G}) &=& \Delta(G) \\
								\nonumber \delta(\overline{G}) &=& \delta(G) + 1
							\end{eqnarray}
							\par Repitiendo este proceso, \textit{i.e} $G \rightarrow \overline{G} \rightarrow \overline{\overline{G}} \rightarrow \overline{\overline{\overline{G}}} \dotsc $ eventualmente llegaremos a un $G^{\blacktriangle}$ tal que $\delta(G^{\blacktriangle}) = \Delta \; \therefore $ regular.
					\end{enumerate}
			\textbf{Fin del Lema Interno}

			\vspace{5mm}
			\begin{itemize}
				\item Sea H bipartito regular con $G \subseteq H$ y $\Delta(G) = \Delta(H)$. Como H es bipartito regular $\Rightarrow \exists$ un matching perfecto en H, llamado M. Coloreemos todos los lados de M con \textbf{\texttt{Color 1}}.
				\item Sea $H_{1} = H - $ los lados del matching M. Como M es matching perfecto, $H_{1}$ sigue siendo regular y $\delta_{H_{1}}(x) = \delta_{H}(x) - 1 = \Delta - 1$. Como $H_{1}$ es bipartito regular $\Rightarrow \exists$ un matching perfecto en $H_{1}$, llamado $M_{1}$. Coloreemos todos los lados de $M_{1}$ con \textbf{\texttt{Color 2}}.
				\item Sea $H_{2} = H_{1} - $ los lados del matching $M_{1}$. Luego $\delta_{H_{2}}(x) = \delta_{H_{1}}(x) - 1 = \Delta - 2$.
					\par Siguiendo así $H_{\Delta - 1}$ seguirá siendo regular con $ \delta_{H_{\Delta - 1}}(x) = 1$. En total obtuvimos $ \Delta $ matchings y $ \Delta $ colores. $\therefore \chi '(H) = \Delta$
					\begin{center}
						$\Rightarrow \chi ' (G) = \Delta$ pues $G \subseteq H$.
					\end{center}
			\end{itemize}


	\section{Cota de Hamming}
		\textbf{\underline{Teorema:}} Sea C un código de longitud \textit{n} y sea $ t = \lfloor \frac{\delta - 1}{2} \rfloor$ la cantidad de errores que corrigue, entonces:
			\begin{eqnarray}
				\nonumber \lvert C \rvert &\leq & \frac{2^{n}}{1 + n + {n \choose 2} + \dotsc {n \choose t}}
			\end{eqnarray}

		\textbf{\underline{Prueba:}} Sea \begin{tabular}{|c|} \hline $ A = \cup_{x \in C} B_{t}(x) $ \\ \hline \end{tabular}

			\vspace{3mm}
				\par Como ya dijimos C corrigue \textit{t} errores $\Rightarrow B_{t}(x) \cap B_{t}(y) \neq \emptyset \; \forall x, y \in C : x \neq y$.

			\begin{center}
				$\therefore$ La unión en A es disjunta y \begin{tabular}{|c|} \hline $ \#A = \sum_{x \in C} B_{t}(x) $ \\ \hline \end{tabular}
			\end{center}

			\vspace{3mm}
			\par Sea $S_{r}(x) = \left\lbrace y \in Z_{2}^{n} : d(x, y) = r \right\rbrace$

			\vspace{3mm}
			\par $\therefore B_{t}(x) = \cup_{r = 0}^{t}S_{r}(x)$ y la unión es disjunta $\Rightarrow$ \begin{tabular}{|c|} \hline $ \#B_{t}(x) = \sum_{r = 0}^{t}\#S_{r}(x) $ \\ \hline \end{tabular}

			\vspace{3mm}
			\textbf{¿Cuánto vale $S_{r}(x)$?}
			\begin{eqnarray}
				\nonumber y \in S_{r}(x) & \Leftrightarrow & d(x, y) = r \\
				\nonumber & \Leftrightarrow & \lvert x \oplus y \rvert = r \text{ donde defino } (x \oplus y) = \epsilon \\
				\nonumber & \Leftarrow & \exists \underbrace{\epsilon : \lvert \epsilon \rvert = r}_{\in S_{r}(0)} : y = x \oplus \epsilon
			\end{eqnarray}
			\par $\therefore \; S_{r}(x) = x \oplus S_{r}(0) \Rightarrow \#S_{r}(x) = \#S_{r}(0)$

			\vspace{3mm}
			\par Luego
			\begin{eqnarray}
				\nonumber \#S_{r}(x) &=& \#S_{r}(0) \\
				\nonumber &=& \# \text{ vectores de longitud n con r unos } \\
				\nonumber &=& {n \choose r}
			\end{eqnarray}

			\par Por lo tanto
			\begin{eqnarray}
				\nonumber \# A &=& \sum_{x \in C} \# B_{t}(x) \\
				\nonumber &=& \sum_{x \in C} \; \sum_{r= 0}^{t} \#S_{r}(x) \\
				\nonumber &=& \sum_{x \in C} \; \sum_{r = 0}^{t} {n \choose r} \\
				\nonumber &=& \# C * \sum_{r = 0}^{t} {n \choose r}
			\end{eqnarray}

			\par Como $A \in Z_{2}^{n} \Rightarrow \# A \leq 2^{n}$, entonces:
			\begin{eqnarray}
				\nonumber \lvert \sum_{r = 0}^{t} {n \choose r} \rvert * \# C \leq 2^{n} \\
				\nonumber \Rightarrow \# C \leq \frac{2^{n}}{\sum_{r = 0}^{t} {n \choose r}}
			\end{eqnarray}


	\section{Teorema de la matriz de chequeo de códigos lineales}
		\textbf{\underline{Teorema:}} Sea C un código lineal con matriz de chequeo H, entonces:
			\begin{itemize}
				\item[a)] $\delta(C) = \min \left\lbrace \lvert x \rvert : x \in C , x \neq 0 \right\rbrace (\#$ columas LD de H).
				\item[b)] Si H no tiene columnas 0, ni columnas repetidas, entonces $\delta \geq 3$ y corrigue al menos un error.
			\end{itemize}

		\textbf{\underline{Prueba:}} Sean:
			\begin{itemize}
				\item $w : \min \; \#$ columnas LD de H.
				\item $e_{i} = 000 \dotsc \underbrace{1}_{i} 0 \dotsc 0$
			\end{itemize}

			\textbf{a)} \begin{tabular}{|c|} \hline $w \leq \delta \; $ \\\hline \end{tabular} $ \; \; Sea \; v \in C , v \neq 0 \; y \; \lvert v \rvert = \delta \Rightarrow v$ tiene $\delta$ unos.

			\vspace{3mm}
			\par Es decir; $\exists j_{1}, j_{2} \dotsc j_{\delta}$ tal que:
			\begin{eqnarray}
				\nonumber v = e_{j_{1}} + e_{j_{2}} + \dotsc e_{j_{\delta}}
			\end{eqnarray}

			\par Como $ v \in C \Rightarrow v =$ NU(H) entonces $Hv^{t} = 0$.
			\begin{eqnarray}
				\nonumber \Rightarrow 0 = Hv^{t} &=& H(e_{j_{1}}^{t} + e_{j_{2}}^{t} + \dotsc e_{j_{\delta}}^{t}) \\
				\nonumber &=& H e_{j_{1}}^{t} + H e_{j_{2}}^{t} + \dotsc H e_{j_{\delta}}^{t} \\
				\nonumber &=& H^{j_{1}} + H^{j_{2}} + \dotsc H^{j_{\delta}}
			\end{eqnarray}
			\par $\therefore \left\lbrace H^{j_{1}} + H^{j_{2}} + \dotsc H^{j_{\delta}} \right\rbrace$ es LD $\Rightarrow w \leq \delta$.

			\vspace{3mm}
			\begin{tabular}{|c|} \hline $w \geq \delta $ \\\hline \end{tabular} Sea ahora $\left\lbrace H^{i_{1}} + H^{i_{2}} + \dotsc H^{i_{w}} \right\rbrace$ un conjunto LD, entonces $\exists$ constantes $c_{w} \in Z_{2}^{n}$ no todas nulas, tales que:

			\begin{eqnarray}
				\nonumber c_{1} H^{i_{1}} + c_{2} H^{i_{2}} + \dotsc c_{w} H^{i_{w}} = 0
			\end{eqnarray}

			\par Sea $v = c_{1} e_{i_{1}} + c_{2} e_{i_{2}} + \dotsc c_{w} e_{i_{w}} , \; v \neq 0$ ya que dijimos que no todos los $c_{i}$ eran nulos.
			\par Luego:
			\begin{eqnarray}
				\nonumber H v^{t} &=& H (c_{1} e_{i_{1}}^{t} + c_{2} e_{i_{2}}^{t} + \dotsc c_{w} e_{i_{w}}^{t}) \\
				\nonumber &=& H c_{1} e_{i_{1}}^{t} + H c_{2} e_{i_{2}}^{t} + \dotsc H c_{w} e_{i_{w}}^{t} \\
				\nonumber &=& H^{e_{i_{1}}} + H^{e_{i_{2}}} + \dotsc H^{e_{i_{w}}} \\
				\nonumber &=& 0
			\end{eqnarray}
			\par $\Rightarrow v \in C \; y \; \lvert v \rvert = w$. Como $\delta$ es la menor distancia, tenemos que $\delta \leq \lvert v \rvert = w$.

			\vspace{5mm}
			\par Por lo tanto vale \begin{tabular}{|c|} \hline $w \leq \delta $ \\ \hline \end{tabular} y \begin{tabular}{|c|} \hline $w \leq \delta $ \\ \hline \end{tabular} $\Rightarrow$ \begin{tabular}{|c|} \hline $w = \delta $ \\ \hline \end{tabular}.

			\vspace{5mm}
			\textbf{b)}
			\begin{itemize}
				\item Si H no tiene columnas ceros $\Rightarrow w \leq 2$.
				\item Si H no tiene columnas repetidas $\Rightarrow \nexists i,j : i \neq j$ y $H^{i} = H^{j}$. Como no puede pasar que $H{i} = H{j}$ tampoco $H{i} + H{j} = 0$, es decir, $H{i} + H{j} \neq 0 \; \forall i,j : i \neq j$ por lo que $\nexists$ conjuntos de columnas LD $\Rightarrow w \geq 3$.

				\par Luego $\delta \geq 3 \Rightarrow t = \lfloor \frac{\delta-1}{2} \rfloor \geq 1$, es decir, C corrigue al menos un error.
			\end{itemize}


	\section{Teorema del polinomio generador de códigos cíclicos}
		\textbf{\underline{Teorema:}} Sea C un código cíclico de dimensión \textit{k} y longitud \textit{n} y sea $g(x)$ su polinomio generador, entonces:
			\begin{itemize}
				\item[a)] C está formado por los múltiplos de $g(x)$ de grado menor a \textit{n}.
				\item[b)] El grado de $g(x)$ es $n - k$.
				\item[c)] $g(x)$ divide a $1 + x^{n}$
			\end{itemize}

		\textbf{\underline{Prueba:}}
			\par \textbf{a)} Sea $v(x) \in C$, dividamos v por g. Entonces $\exists$ q y r con $gr(r) < gr(g)$ tal que:
			\begin{eqnarray}
				\nonumber v &=& q g + r \\
				\nonumber & \Rightarrow & \begin{tabular}{|c|} \hline $q g = v + r $ \\ \hline \end{tabular} (1)
			\end{eqnarray}.

			\par Como $v \in C \Rightarrow gr(v) < n$ y $gr(r) < gr(g) \underbrace{<}_{g \in C} n $ con lo que concluimos que:
			\begin{center}
				\begin{tabular}{|c|} \hline $gr(v + r) < n $ \\ \hline \end{tabular} (2).
			\end{center}
			\par Luego de (1) y (2) deducimos: \begin{tabular}{|c|} \hline $gr(q \; g) < n $ \\ \hline \end{tabular}.

			\vspace{3mm}
			\par Recordemos que si $p \in C$ y $gr(p) < n$ entonces: \begin{tabular}{|c|} \hline $p mod (1+ x^{n}) = p$ \\\hline \end{tabular}.

			\vspace{3mm}
			\par Por lo tanto:
			\begin{center}
				$ q\; g \; mod \; (1 + x^{n}) = q \; g $ \\
				\vspace{3mm}
				i.e \begin{tabular}{|c|} \hline $ q \odot g = q \; g $ \\\hline \end{tabular} (A).
			\end{center}
			\par Además como $g(x) \in C \Rightarrow$ \begin{tabular}{|c|} \hline $q(x) \odot g(x) \in C $ \\ \hline \end{tabular} (B).

			\vspace{3mm}
			\par De (A) y (B) resuta que $q \; g \in C \Rightarrow v + r \in C$. Además dijimos que $v \in C$ y como C es lineal $\Rightarrow$ \begin{tabular}{|c|} \hline $r \in C$ \\\hline \end{tabular} $(\dag)$.
			\vspace{3mm}
			\par Llamemos \begin{tabular}{|c|} \hline $gr(r) < gr(g) $ \\ \hline \end{tabular} $(\star)$.

			\vspace{5mm}
			\par De $(\dag)$ y $(\star)$ deducimos $r = 0 \Rightarrow v = q \; g$.

			\vspace{5mm}
			\textbf{b)} Recordemos que si $ v(x) \in C \Rightarrow \exists \mu : v = \mu g$. Entonces para que $gr(\mu g) < n$ debe darse que $gr(\mu) + gr(g) < n$, es decir, $gr(\mu) < n - gr(g)$.

			\vspace{3mm}
			\par Sea
			\begin{eqnarray}
				\nonumber C = \left\lbrace v : \exists \mu, gr(\mu) < n - gr(g), v = \mu g \right\rbrace
			\end{eqnarray}

			\par Entonces existe una biyección entre C y el conjunto: $\left\lbrace \mu :  gr(\mu) < n - gr(g) \right\rbrace$.
			\begin{eqnarray}
				\nonumber \therefore \; \lvert C \rvert &=& \lvert \left\lbrace \mu : gr(\mu) < n - gr(g) \right\rbrace \rvert \\
				\nonumber 2^{k} &=& 2^{n - gr(g)} \\
				\nonumber k &=& n - gr(g) \\
				\nonumber gr(g) &=& n - k
			\end{eqnarray}

			\textbf{c)} Recordemos que si \textit{g} es polinomio generador, la $ROT^{k}(g) = x^{k} \odot g$.
			\vspace{5mm}
			\par Como $g(x) \in C \Rightarrow ROT^{k}(g) \in C$. Además $x^{k} \odot g = x^{k} g mod (1 + x^{n}).$
			\vspace{3mm}
			\par Sea $p(x)$ tal que:
			\begin{eqnarray}
				\nonumber \underbrace{x^{k} g}_{gr = k + (n - k)} = \underbrace{p(x) (1 + x^{n})}_{gr = gr(p) + n} + \underbrace{x^{k} \odot g}_{gr < n}
			\end{eqnarray}
			\par Luego para que la igualdad valga, debe valer que $gr(p)	= 0$, es decir, $p = 1$. Entonces:
			\begin{center}
				\begin{tabular}{|c|} \hline $ x^{k} g = (1 + x^{n}) + x^{k} \odot g $ \\\hline \end{tabular} (1)
			\end{center}

			\par pero $x^{k} \odot \in C \Rightarrow \exists \mu(x) :$
			\begin{center}
				\begin{tabular}{|c|} \hline $ x^{k} \odot g = \mu(x) g $ \\\hline \end{tabular} (2)
			\end{center}

			\vspace{3mm}
			\par De (1) y (2):
			\begin{eqnarray}
				\nonumber x^{k} g &=& (1 + x^{n}) + \mu g \\
				\nonumber 1 + x^{n} &=& x^{k} g+ \mu g \\
				\nonumber &=& (x^{k} + \mu) g \\
				\nonumber & \therefore & g | 1 + x^{n}
			\end{eqnarray}



\chapter{Parte C}

	\section{4-COLOR $\leq_{\textit{p}}$ SAT}
		\textbf{\underline{Teorema:}} 4-COLOR $\leq_{p}$ SAT.

		\textbf{\underline{Prueba:}} Sea G un grafo con vértices $v_{1}, v_{2}, \dotsc v_{n}$, queremos construir (en tiempo polinomial) una expresión booleana B en CNF (\textit{forma conjuntiva normal}), tal que:
			\[ \chi(G) \leq 4 \Leftrightarrow \textup{ B es satisfacible} \]
			\par Sea n $=$ \#vértices de G, y sean las variables $x_{i, j} \textup{ con } i = 1, 2, \dotsc n$ y $j = 1, 2, 3, 4$

			\vspace{3mm}
			\par Sean
			\begin{eqnarray}
				\nonumber A_{i} &=& x_{i, 1} \vee x_{i, 2} \vee x_{i, 3} \vee x_{i, 4}  \\
				\nonumber A &=& A_{1} \wedge A_{2} \wedge \dotsc A_{n}
			\end{eqnarray}

			\par Defino
			\begin{eqnarray}
				Q_{i, j, k, l} &=& \overline{x_{i, j}} \vee \overline{x_{k, l}}  \qquad i, k = 1 \dotsc n \qquad j, l = 1, 2, 3, 4 \\
				\nonumber \\
				D_{i} &=& Q_{i, i, 1, 2} \wedge Q_{i, i, 1, 3} \wedge Q_{i, i, 1, 4} \wedge Q_{i, i, 2, 3} \wedge Q_{i, i, 2, 4} \wedge Q_{i, i, 3, 4} \\
				\nonumber \\
				D &=& D_{1} \wedge D_{2} \dotsc \wedge D_{n} \\
				\nonumber \\
				F_{i, k} &=& Q_{i, k, 1, 1} \wedge Q_{i, k, 2, 2} \wedge Q_{i, k, 3, 3} \wedge Q_{i, k, 4, 4} \\
				\nonumber \\
				F &=& \wedge_{\begin{subarray}{l} \; i, k \\ \; v_{i} v_{k} \in E(G) \end{subarray}} F_{i, k}
			\end{eqnarray}

			\begin{tabular}{|c|} \hline $\Rightarrow$ \\ \hline \end{tabular}
				\vspace{3mm}

				\par Sea C un coloreo propio de G, con a lo sumo 4 colores. Debemos dar un asignamiento de valores a las variables $x_{i, j}$ que satisfagan B, es decir, un elemento $\overrightarrow{b} \in \mathbb{Z}^{4n}_{2}$ tal que $B(\overrightarrow{b}) = 1$.

				\vspace{5mm}
				\par Definimos
				\begin{equation*}
					b_{i, j}
		  			\left \lbrace
		  			\begin{array}{l}
		    		 1 \textup{ si } C(v_{i}) = j \\
		     		 0 \; c.c \\
		  			\end{array}
		  			\right.
				\end{equation*}
				\par Debemos probar $B(\overrightarrow{b}) = 1$, es decir, (1) $A(\overrightarrow{b}) = 1$, (2) $D(\overrightarrow{b}) = 1$ y (3) $F(\overrightarrow{b}) = 1$.

				\vspace{3mm}
				(1) \begin{tabular}{|c|} \hline $A(\overrightarrow{b}) = 1$ \\ \hline \end{tabular}
					\par Debemos ver que $A_{i}(\overrightarrow{b}) = 1 \; \forall i$, es decir, hay que probar:
					\[ b_{i, 1} \vee b_{i, 2} \vee b_{i, 3} \vee b_{i, 4} = 1 \]
					pero
					\[ C(v_{i}) \in \lbrace 1, 2, 3, 4 \rbrace \Rightarrow \exists j \in \lbrace 1, 2, 3, 4 \rbrace \textup{ tal que } C(v_{i}) = j \Rightarrow b_{i, j} = 1\]

				\vspace{3mm}
				(2) \begin{tabular}{|c|} \hline $D(\overrightarrow{b}) = 1$ \\ \hline \end{tabular}
					\par Debemos ver que $D_{i}(\overrightarrow{b}) = 1 \; \forall i$, es decir,
					\[ Q_{i, j, j, l}(\overrightarrow{b}) = 1 \; \forall i, j, l,  j < l \] es decir:

					\[ \overline{b_{i, j}} \vee \overline{b_{i, l}} = 1 \; \forall i, j, l, j <  l\]
					por el absurdo, supongamos que no:
					\begin{eqnarray}
						\nonumber &\Rightarrow & \exists i, j, l, j < l \; : \; \overline{b_{i, j}} \vee \overline{b_{i, l}} = 0 \\
						\nonumber &\Rightarrow & \overline{b_{i, j}} = \overline{b_{i, l}} = 0 \\
						\nonumber &\Rightarrow & b_{i, j} = b_{i, l} = 1 \\
						\nonumber &\Rightarrow & C(v_{i}) = j, \; C(v_{i}) = l, \; pero \; j \neq l \; \textup{Absurdo!}
					\end{eqnarray}
					\par Luego, el absurdo vino de suponer que $\overline{b_{i, j}} \vee \overline{b_{i, l}} = 0$

					\vspace{3mm}
					$\therefore \overline{b_{i, j}} \vee \overline{b_{i, l}} = 1$

				\vspace{3mm}
				(3) \begin{tabular}{|c|} \hline $F(\overrightarrow{b}) = 1$ \\ \hline \end{tabular}
				\vspace{3mm}

					\par Por el absurdo, supongamos que no:
					\begin{eqnarray}
						\nonumber &\Rightarrow & \exists i, k \; : \; v_{i} v_{k} \in E(G) \textup{ tal que } F_{i, k}(\overrightarrow{b}) = 0 \\
						\nonumber &\Rightarrow & \exists j \; \in \lbrace 1, 2, 3, 4 \rbrace : Q_{i, k, j, j}(\overrightarrow{b}) = 0 \\
						\nonumber &\Rightarrow & \overline{b_{i, j}} \vee \overline{b_{k, j}} = 0 \\
						\nonumber &\Rightarrow & \overline{b_{i, j}} = \overline{b_{k, j}} = 0 \\
						\nonumber &\Rightarrow & b_{i, j} = b_{k, j} = 1 \\
						\nonumber &\Rightarrow & C(v_{i}) = 1\; y \; C(v_{k}) = 1\\
						\nonumber &\Rightarrow & C(v_{i}) = C(v_{k}) \; (= j) \; \textup{Absurdo!}
					\end{eqnarray}
					\par Pues, el coloreo C es propio y $v_{i} v_{k} \in E(G)$. El absurdo vino de suponer que $F(\overrightarrow{b}) = 0$

					\vspace{3mm}
					\par $\therefore F(\overrightarrow{b}) = 1$

			\vspace{5mm}
			\begin{tabular}{|c|} \hline $\Leftarrow$ \\ \hline \end{tabular}

				\par Ahora sabemos que existe un $\overrightarrow{b}$ con $B(\overrightarrow{b}) = 1$ y debemos construir un coloreo propio, con al menos 4 colores.
				\begin{equation*}
					B(\overrightarrow{b}) = 1\Rightarrow
		  			\left \lbrace
		  			\begin{array}{l}
		    		 A(\overrightarrow{b}) = 1 \Rightarrow A_{i}(\overrightarrow{b}) = 1 \; \forall i \Rightarrow  (1) \; \forall i \; \exists j : b_{i, j}  = 1 \\
		     		 D(\overrightarrow{b}) = 1 \Rightarrow D_{i}(\overrightarrow{b}) = 1 \; \forall i \Rightarrow  (2) \;  \forall i \; \nexists j \neq l : b_{i, j}  = 1 = b_{i, l} \\
		     		 F(\overrightarrow{b}) = 1 \Rightarrow \forall i, j \in E(G), C(v_{i}) \neq C(v_{j}) \Rightarrow \textup{C es propio}
		  			\end{array}
		  			\right.
				\end{equation*}
				\par De (1) y (2) $\Rightarrow$ \begin{tabular}{|c|} \hline $\forall i \; \exists ! j : b_{i, j} = 1$ \\ \hline \end{tabular}

				\vspace{3mm}
				\par Luego, definimos:
				\[ C(v_{i}) = \textup{unico j con } b_{i, j} = 1\]


	\section{3-SAT es NP-Completo}
		\underline{Teorema:} 3-SAT es NP-Completo.

		\vspace{3mm}
		\underline{Prueba:} Probaremos que SAT $\leq_{p}$ 3-SAT, pues 3-SAT es NP Completo.

		\vspace{3mm}
		Sea
		\begin{eqnarray}
			B &=& D_{1} \wedge D_{2} \dotsc \wedge D_{n} \textup{ con variables } x_{1}, \dotsc x_{n} \\
			\nonumber \\
			D_{i} &=& L_{i, 1} \vee L_{i, 2} \dotsc \vee L_{i, k_{i}} \\
			\nonumber \\
			l_{i, j} &=& \textup{literales}
		\end{eqnarray}

		Para cada$D_{i}$ construiremos $E_{i}$, que serán conjunciones de disyunciones de 3 literales, con variables extras, y luego tomaremos:

		\[ B' = E_{1} \wedge E_{2} \dotsc \wedge E_{n} \]

		\underline{Construcción:}
		\begin{itemize}
			\item Si D tiene 3 literales, es decir, $K_{i} = 3 \Rightarrow E_{i} = D_{i}$
			\item Si D tiene 2 literales, es decir, $K_{i} = 2 \Rightarrow$ tomo una variable extra $y_{i, 1}$ y defino:
				\[ E_{i} = (l_{i, 1} \vee l_{i, 2} \vee y_{i, 1}) \wedge (l_{i, 1} \vee l_{i, 2} \vee \overline{y_{i, 1}}) \]
			\item Si D tiene 1 literales, es decir, $K_{i} = 1 \Rightarrow$ tomo dos variables extras $y_{i, 1}$ y $y_{i, 2}$ y defino:
				\[ E_{i} = (l_{i, 1} \vee y_{i, 1} \vee y_{i, 2}) \wedge (l_{i, 1} \vee \overline{y_{i, 1}} \vee y_{i, 2}) \wedge (l_{i, 1} \vee y_{i, 1} \vee \overline{y_{i, 2}}) \wedge (l_{i, 1} \vee \overline{y_{i, 1}} \vee \overline{y_{i, 2}}) \]
			\item Si D tiene más de 3 literales, es decir, $K_{i} > 4 \Rightarrow$ agrego $k_{i-3}$ variables extras y defino:
				\[ E_{i} = (l_{i, 1} \vee l_{i, 2} \vee y_{i, 1}) \; \wedge \; (l_{i, 3} \vee \overline{y_{i, 1}} \vee y_{i, 2}) \; \wedge \; (l_{i, 4} \vee \overline{y_{i, 2}} \vee y_{i, 3}) \; \wedge \]
				\[ \dotsc (l_{i, k_{i-2}} \vee \overline{y_{i, k_{i-4}}} \vee y_{i, k_{i-3}}) \wedge (l_{i, k_{i-1}} \vee l_{i, k_{i}} \vee \overline{i, k_{i-3}}) \]
		\end{itemize}

		Queremos ver:
		\[ B(\overrightarrow{b}) = 1 \Leftrightarrow \exists \overrightarrow{a} : B'(\overrightarrow{b}, \overrightarrow{a}) = 1 \]

		\vspace{5mm}
		\begin{tabular}{|c|} \hline $\Leftarrow$ \\ \hline \end{tabular}

		\underline{Casos:}
			\begin{itemize}
				\item si $k_{i} = 3$ Absurdo! pues $D_{i} = E_{i}$
				\item si $k_{i} = 2$
					\begin{eqnarray}
						\nonumber 1 &=& E_{i}(\overrightarrow{b}, \overrightarrow{a}) \\
						\nonumber &=& (\underbrace{l_{i, 1}(\overrightarrow{b}) \vee l_{i, 2}(\overrightarrow{b})}_{= 0} \vee a_{i, 1}) \wedge (\underbrace{l_{i, 1}(\overrightarrow{b}) \vee l_{i, 2}(\overrightarrow{b})}_{= 0} \vee \overline{a_{i, 1}}) \\
						\nonumber &=& a_{i, 1} \wedge \overline{a_{i, 1}} \\
						\nonumber &=& 0 \textup{ Absurdo! }
					\end{eqnarray}
				\item si $k_{i} = 1$
					\begin{eqnarray}
						\nonumber 1 &=& E_{i}(\overrightarrow{b}, \overrightarrow{a}) \\
						\nonumber &=& (a_{i, 1} \vee a_{i, 2}) \wedge (\overline{a_{i, 1}} \vee a_{i, 2}) \wedge (a_{i, 1} \vee \overline{a_{i, 2}}) \wedge (\overline{a_{i, 1}} \vee \overline{a_{i, 2}}) \\
						\nonumber &=& 0 \textup{ Absurdo! }
					\end{eqnarray}
				\item si $k_{i} > 4$

					$D_{i}(\overrightarrow{b}) = 0 \Rightarrow l_{i, j}(\overline{b}) = 0 \; \forall j$.

					\begin{eqnarray}
						\nonumber 1 &=& E_{i}(\overrightarrow{b}, \overrightarrow{a}) \\
						\nonumber &=& a_{i, j} \wedge (\overline{a_{i, 1}} \vee a_{i, 2}) \wedge (\overline{a_{i, 2}} \vee a_{i, 3}) \wedge \dotsc  	(\overline{a_{i, k_{i-4}}} \vee a_{i, k_{i-3}}) \wedge \overline{a_{i, k_{i-3}}} \\
						\nonumber &=& a_{i, 1} \wedge (a_{i, 1} \Rightarrow a_{i, 2}) \wedge (a_{i, 2}\Rightarrow a_{i, 3}) \wedge 	\dotsc (a_{i, k_{i-4}} \Rightarrow a_{i, k_{i-3}}) \wedge \overline{a_{i, k_{i-3}}} \textup{ Absurdo! }
					\end{eqnarray}

					Pues tendría $a_{i, 1} = a_{i, 2} = \dotsc a_{i, k_{i-3}} = 1$ pero $\overline{a_{i, k_{i-3}}} = 1$.
			\end{itemize}

		\vspace{5mm}
		\begin{tabular}{|c|} \hline $\Rightarrow$ \\ \hline \end{tabular}

			\underline{Casos:}
			\begin{itemize}
				\item Si $k_{i} = 3 \Rightarrow E_{i}(\overrightarrow{b}, \overrightarrow{a}) = D_{i}(\overrightarrow{b}) = 1$
				\item Si $k_{i} = 1 \textup{ ó } k_{i} = 2 \textup{ defino } a_{i, j} = 0, \textup{ luego } E_{i}(\overrightarrow{b}, 0) = 1$
				\item Si $k_{i} > 4 \Rightarrow D_{i}(\overrightarrow{b}) = 1 \Rightarrow \exists j : l_{i, j}(\overrightarrow{b}) = 1$

					Definimos
						\[ a_{i, 1} = a_{i, 2} = \dotsc a_{i, j-2} = 1 \]
						\[ a_{i, j-1} = a_{i, j} = \dotsc a_{i, k-3} = 0 \]

					Luego
					\begin{eqnarray}
						\nonumber E_{i}(\overrightarrow{b}, \overrightarrow{a}) &=& (l_{i, 1}(\overrightarrow{b}) \vee l_{i, 2}(\overrightarrow{b}) \vee a_{i, 1}) \\
						\nonumber &\wedge & (l_{i, 3}(\overrightarrow{b}) \vee \overline{a_{i, 1}} \vee a_{i, 2}) \\
						\nonumber &\vdotswithin{\ldots}& \\
						\nonumber &\wedge & (l_{i, j-1}(\overrightarrow{b}) \vee \overline{a_{i, j-3}} \vee a_{i, j-2}) \\
						\nonumber &\wedge & (l_{i, j}(\overrightarrow{b}) \vee \overline{a_{i, j-2}} \vee a_{i, j-1}) \\
						\nonumber &\wedge & (l_{i, j+1}(\overrightarrow{b}) \vee \overline{a_{i, j-1}} \vee a_{i, j}) \\
						\nonumber &\vdotswithin{\ldots}& \\
						\nonumber &\wedge & (l_{i, k_{i-1}}(\overrightarrow{b}) \vee l_{i, k_{i}}(\overrightarrow{b}) \vee \overline{a_{i, k_{i-3}}}) \\
						\nonumber &\therefore & E_{i} = 1
					\end{eqnarray}
			\end{itemize}

	\section{3-COLOR es NP-Completo}
		\underline{Teorema:} 3-COLOR es NP-Completo

		\underline{Prueba:} Veremos que $3-SAT \leq_{p} 3-Color$, es decir, dado B en CNF con 3 literales por disjunción, debemos crear un grafo G, tal que:

			\[ \textup{B es satisfacible } \Leftrightarrow \chi(G) \leq 3 \]

		\par Sea:
			\begin{eqnarray}
				\nonumber B &=& D_{1}, D_{2} \dotsc D_{m} \textup{ con variables } \lbrace x_{1} \dotsc x_{m} \rbrace \\
				\nonumber D_{i} &=& l_{i, 1} \vee l_{i, 2} \vee l_{i, 3}
			\end{eqnarray}

		\vspace{3mm}
		\underline{Construcción del grafo:}
			\begin{itemize}
				\item Dado un literal \textit{l}, definimos:
					\begin{equation*}
						\psi(l) =
  						\left \lbrace
		  				\begin{array}{l}
    				 	v_{k} \; \textup{ si } l = x_{k} \\
    		 			w_{k} \textup{ si } l = \overline{x_{k}}
		  				\end{array}
  						\right.
					\end{equation*}
				\item Vértices:
					\[ V(G) = \lbrace s, t \rbrace \cup \lbrace v_{1} \dotsc v_{n}, w_{1} \dotsc w_{n} \rbrace \; \cup \]
					\[ \qquad \qquad \; \; \; \; \lbrace \mu_{i, 1}, \mu_{i, 2}, \mu_{i, 3}, b_{i, 1}, b_{i, 2}, b_{i, 3} \rbrace_{i = 1}^{m}  \]
					\item Aristas:
						\[ \lbrace s t \rbrace \cup \lbrace t v_{i}, t w_{i}, v_{i} w_{i} \rbrace_{i = 1}^{n} \; \cup \lbrace s \mu_{i, j} \rbrace_{i = 1, \; \; j = 1, 2, 3}^{m} \cup F \]
						\[ \lbrace b_{i, 1} b_{i, 2}, b_{i, 1} b_{i, 3}, b_{i, 2} b_{i, 3}, b_{i, 1} \mu_{i, 1}, b_{i, 2}, \mu_{i, 2}, b_{i, 3} \mu_{i, 3} \rbrace_{i = 1}^{m}\]
						\par Donde:
							\[ F = \lbrace \mu_{i, j} \psi(l_{i, j}) \rbrace_{i = 1, \; \; j = 1, 2, 3}^{m}\]
			\end{itemize}

			\begin{equation*}
						\textup{ G tiene: }
  						\left \lbrace
		  				\begin{array}{l}
    				 		2 + 2n + 6m \textup{ vertices} \\
    				 		1 + 3n + 6m+ 3m + 3m \textup{ aristas}
		  				\end{array}
  						\right\rbrace
  						\Rightarrow \textup{ G es polinomial}
					\end{equation*}



		\vspace{5mm}
		\begin{tabular}{|c|} \hline $\Leftarrow$ \\ \hline \end{tabular}

			Como G tiene triángulos, si $\chi (G) \leq 3$, entonces debe ser $\chi (G) = 3$. Por lo tanto, existe un coloreo C de G con 3 colores.

			Definición:
			\begin{equation*}
				b_{k} =
  				\left \lbrace
  				\begin{array}{l}
    		 	1 \textup{ si } C(v_{k}) = C(s) \\
    		 	0 \textup{ c.c}
  				\end{array}
  				\right.
			\end{equation*}

		Para probar $B(\overrightarrow{b}) = 1$ debemos probar que $D_{i}(\overrightarrow{b}) = \forall i$.

		Sea $i \in \lbrace	1, 2, \dotsc m \rbrace$, como $ \lbrace b_{i, 1}, b_{i, 2}, b_{i, 3} \rbrace$ es un triángulo, entonces deben aparecer los 3 colores.
		\par 	Por lo tanto: $\exists j : C(b_{i, j}) = C(s)$
		\par Luego:
			\begin{equation*}
  			\left.
  			\begin{array}{l}
    		 	(1) \; \mu_{i, j} b_{i, j} \in E(G) \Rightarrow C(\mu_{i, j}) \neq C(b_{i, j}) = C(t) \\
    		 	(2) \; \mu_{i, j} s \in E(G) \Rightarrow C(\mu_{i, j}) \neq C(s) \\
    		 	(3) \; s t \in E(G) \Rightarrow C(s) \neq C(t)
  			\end{array}
 			 \right\rbrace
 			 \Rightarrow C(\mu_{i, j}) = \textup{ TERCER COLOR}
			\end{equation*}

		\par Por otro lado:
			\begin{equation*}
  			\left.
  			\begin{array}{l}
    		 	(1) \; \mu_{i, j} \psi(l_{i, j}) \in E(G) \Rightarrow C(\psi(l_{i, j}) \neq \textup{ TERCER COLOR} \\
    		 	(2) \; \psi(l_{i, j}) t \in E(G) \Rightarrow C(\psi(l_{i, j}) \neq C(t)
  			\end{array}
 			 \right\rbrace
 			 \Rightarrow C(\psi(l_{i, j}) = C(s)
			\end{equation*}

		\begin{itemize}
			\item Caso (1):
				\begin{eqnarray}
						\nonumber l_{i, j} = x_{k} &\Rightarrow & l_{i, j}(\overrightarrow{b}) = b_{k} \\
						\nonumber &\Rightarrow& \psi(l_{i, j}) = v_{k}
				\end{eqnarray}
				\par Entonces:
				\begin{eqnarray}
						\nonumber C(v_{k}) &=& C(s) \Rightarrow b_{k} = 1 \\
						\nonumber \therefore l_{i, j}(\overrightarrow{b}) &=& 1 \Rightarrow D_{i}(\overrightarrow{b}) = 1
				\end{eqnarray}
			\item Caso (2):
				\begin{eqnarray}
						\nonumber l_{i, j} = \overline{x_{k}} &\Rightarrow & l_{i, j}(\overrightarrow{b}) = 	\overline{b_{k}} \\
						\nonumber &\Rightarrow& \psi(l_{i, j}) = w_{k}
				\end{eqnarray}
				\par Entonces:
				\begin{equation*}
  					\left.
  					\begin{array}{l}
    		 			C(w_{k}) = C(s) \\
    		 			v_{k} w_{k} \in E(G)
  					\end{array}
 			 		\right\rbrace
 			 		\Rightarrow C(v_{k}) \neq C(s) \therefore b_{k} = 0
				\end{equation*}
				\[ \therefore b_{k} = 1 \Rightarrow D_{i}(\overrightarrow{b}) = 1 \]
		\end{itemize}

		\vspace{5mm}
		\begin{tabular}{|c|} \hline $\Rightarrow$ \\ \hline \end{tabular}
			\par Acá asumimos que $\exists \overrightarrow{b} : B(\overrightarrow{b}) = 1 \Rightarrow \forall i, \; D(\overrightarrow{b}) = 1 \Rightarrow$ \begin{tabular}{|c|} \hline $\forall i \exists j : l_{i, j}(\overrightarrow{b}) = 1$ \\ \hline \end{tabular} $(\star)$. Debemos construir un coloreo propio con 3 colores.
			\par Definimos:
			 	\begin{equation*}
  					\left.
  					\begin{array}{l}
    		 			C(s) = \textup{ BLANCO} \\
    		 			C(t) = \textup{ AZUL}
  					\end{array}
 			 		\right\rbrace
 			 		\Rightarrow s t \textup{ No Crea Problemas (NCP)}
				\end{equation*}
				\begin{equation*}
					C(v_{k}) =
		  			\left\lbrace
  					\begin{array}{l}
    		 			\textup{BLANCO si } b_{k} = 1 \\
    		 			\textup{AZUL si } b_{k} = 0
  					\end{array}
 			 		\right\rbrace
 			 		\Rightarrow \underbrace{v_{k}}_{B \; o \; N} \underbrace{w_{k}}_{B \; o \; N} \textup{ NCP}
				\end{equation*}
				\begin{equation*}
					C(w_{k}) =
		  			\left\lbrace
  					\begin{array}{l}
    		 			\textup{NEGRO \; \, si } b_{k} = 1 \\
    		 			\textup{BLANCO \; si } b_{k} = 0
  					\end{array}
 			 		\right\rbrace
 			 		\Rightarrow v_{k} t  \textup { y } w_{k} t \textup{ NCP}
				\end{equation*}

		\par Falta colorear las garras. Dado \textit{i}, tomemos el \textit{j} de $(\star )$ y definimos:
				\begin{equation*}
					C(\mu_{i, r}) =
		  			\left\lbrace
  					\begin{array}{l}
    		 			\textup{NEGRO \; \; si } r = j \\
    		 			\textup{BLANCO \; si } r \neq j
  					\end{array}
 			 		\right\rbrace
 			 		\Rightarrow \underbrace{\mu_{i, r}}_{N \; o \; A} \underbrace{s}_{B} \textup{ NCP } \forall r
				\end{equation*}

		\begin{itemize}
			\item Caso $r \neq j : \qquad \underbrace{\mu_{i, r}}_{A} \underbrace{\psi(l_{i, r})}_{B \; o \, N} \Rightarrow$ NCP
			\item Caso $r = j$:
				\begin{itemize}
			\item Caso (1):
				\begin{eqnarray}
						\nonumber l_{i, j} = x_{k} &\Rightarrow & l_{i, j}(\overrightarrow{b}) = b_{k} = 1 \Rightarrow C(v_{k}) = \textup{ BLANCO} \\
						\nonumber &\Rightarrow& \psi(l_{i, j}) = v_{k}	\Rightarrow \mu_{i, j} \psi(l_{i, j}) = \underbrace{\mu_{i, j}}_{N} \underbrace{v_{k}}_{B} \textup{ NCP}
				\end{eqnarray}

			\item Caso (2):
				\begin{eqnarray}
						\nonumber l_{i, j} = \overline{x_{k}} &\Rightarrow & l_{i, j}(\overrightarrow{b}) = \overline{b_{k}} = 1 \Rightarrow b_{k} = 0 \Rightarrow C(w_{k}) = \textup{ BLANCO} \\
						\nonumber &\Rightarrow& \psi(l_{i, j}) = w_{k}	\Rightarrow \mu_{i, j} \psi(l_{i, j}) = \underbrace{\mu_{i, j}}_{N} \underbrace{w_{k}}_{B} \textup{ NCP}
				\end{eqnarray}
			\end{itemize}
		\end{itemize}

	\par Quedan las bases:
		\begin{equation*}
			C(b_{i, r}) =
		  	\left\lbrace
  			\begin{array}{l}
    		 	\textup{AZUL \qquad si } r = j \\
    		 	\textup{BLANCO \; si } r \neq j \\
    		 	\textup{NEGRO \; \; si } r \neq j
  			\end{array}
 			\right.
		\end{equation*}

	Entonces:
	\begin{itemize}
		\item $\lbrace b_{i, 1}, b_{i, 2}, b_{i, 3} \rbrace \textup{ NCP}$
 		\item $\underbrace{b_{i, j}}_{A} \underbrace{\mu_{i, j}}_{N} \textup{ NCP}$
 		\item $\therefore \underbrace{b_{i, r}}_{B \; o \; N} \underbrace{\mu_{i, r}}_{A} \textup{ NCP}$
	\end{itemize}



\begin{thebibliography}{X}
\bibitem{Baz} \textsc{Maximiliano Illbele},
<<Resumen de Discreta II, 16 de agosto de 2012>>,
\textit{FaMAF, UNC}.
\bibitem{Baz} \textsc{Lucia Pappaterra},
<<Resumen de Discreta II, 2014>>,
\textit{FaMAF, UNC}.
\bibitem{Baz} \textsc{Agustín Curto},
<<Carpeta de Clase, 2016>>,
\textit{FaMAF, UNC}.
\end{thebibliography}

\end{document}
